# ğŸš€ SparseAttn

<div align="center">

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)
![PyTorch](https://img.shields.io/badge/pytorch-2.0%2B-red.svg)
![CUDA](https://img.shields.io/badge/CUDA-11.8%2B-green.svg)

**é«˜æ€§èƒ½ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åº“ - ä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æä¾›é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—**

[åŠŸèƒ½ç‰¹æ€§](#-åŠŸèƒ½ç‰¹æ€§) â€¢ [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) â€¢ [API æ–‡æ¡£](#-api-æ–‡æ¡£) â€¢ [æ€§èƒ½åŸºå‡†](#-æ€§èƒ½åŸºå‡†) â€¢ [è´¡çŒ®æŒ‡å—](#-è´¡çŒ®æŒ‡å—)

</div>

## ğŸ“– ç®€ä»‹

SparseAttn æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®¾è®¡çš„é«˜æ€§èƒ½ç¨€ç–æ³¨æ„åŠ›åº“ã€‚é€šè¿‡å…ˆè¿›çš„ç¨€ç–åŒ–æŠ€æœ¯å’Œ GPU ä¼˜åŒ–ï¼Œæ˜¾è‘—é™ä½æ³¨æ„åŠ›è®¡ç®—çš„å†…å­˜æ¶ˆè€—å’Œè®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

### ğŸ¯ ä¸»è¦ä¼˜åŠ¿

- **ğŸ”¥ é«˜æ€§èƒ½**: åŸºäº Triton çš„è‡ªå®šä¹‰ CUDA å†…æ ¸ï¼Œæä¾›æè‡´æ€§èƒ½
- **ğŸ’¾ å†…å­˜é«˜æ•ˆ**: ç›¸æ¯”ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼Œå†…å­˜ä½¿ç”¨é‡å‡å°‘ 80%+
- **ğŸ›ï¸ çµæ´»é…ç½®**: æ”¯æŒå¤šç§ç¨€ç–åŒ–ç­–ç•¥å’Œå‚æ•°è°ƒä¼˜
- **ğŸ”§ æ˜“äºé›†æˆ**: ä¸ Transformers ç”Ÿæ€ç³»ç»Ÿæ— ç¼é›†æˆ
- **ğŸ“Š å¤šç§æ¨¡å¼**: æ”¯æŒé¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„ä¸åŒä¼˜åŒ–ç­–ç•¥

## âœ¨ åŠŸèƒ½ç‰¹æ€§

### ğŸ—ï¸ æ ¸å¿ƒç»„ä»¶

#### 1. **Xattention** - è‡ªé€‚åº”ç¨€ç–æ³¨æ„åŠ›
- åŸºäºé˜ˆå€¼çš„åŠ¨æ€ç¨€ç–åŒ–
- æ”¯æŒå› æœå’Œéå› æœæ³¨æ„åŠ›
- é«˜åº¦ä¼˜åŒ–çš„ Triton å†…æ ¸å®ç°

#### 2. **FlexPrefill** - çµæ´»é¢„å¡«å……ç­–ç•¥
- å—çº§ç¨€ç–æ³¨æ„åŠ›
- è‡ªé€‚åº”å—é€‰æ‹©ç®—æ³•
- æ”¯æŒé•¿åºåˆ—é«˜æ•ˆå¤„ç†

#### 3. **Minference** - è½»é‡çº§æ¨ç†
- å‚ç›´å’Œæ–œçº¿ç¨€ç–æ¨¡å¼
- è‡ªé€‚åº”é¢„ç®—åˆ†é…
- é’ˆå¯¹æ¨ç†é˜¶æ®µä¼˜åŒ–

#### 4. **FullPrefill** - å®Œæ•´é¢„å¡«å……
- åŸºäº FlashInfer çš„é«˜æ•ˆå®ç°
- æ”¯æŒè‡ªå®šä¹‰æ©ç 
- å†…å­˜å’Œè®¡ç®—åŒé‡ä¼˜åŒ–

### ğŸ”§ æŠ€æœ¯ç‰¹æ€§

- **ğŸ¯ æ™ºèƒ½ç¨€ç–åŒ–**: åŸºäºæ³¨æ„åŠ›åˆ†æ•°çš„è‡ªé€‚åº”ç¨€ç–æ¨¡å¼
- **âš¡ GPU åŠ é€Ÿ**: å®Œå…¨åŸºäº CUDA çš„é«˜æ€§èƒ½å®ç°
- **ğŸ§© æ¨¡å—åŒ–è®¾è®¡**: å¯æ’æ‹”çš„æ³¨æ„åŠ›ç»„ä»¶
- **ğŸ“ˆ å¯æ‰©å±•æ€§**: æ”¯æŒä»å°æ¨¡å‹åˆ°è¶…å¤§æ¨¡å‹çš„å„ç§è§„æ¨¡
- **ğŸ”’ æ•°å€¼ç¨³å®š**: ç²¾å¿ƒè®¾è®¡çš„æ•°å€¼è®¡ç®—ç¡®ä¿è®­ç»ƒç¨³å®šæ€§

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ğŸ“‹ ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.8+
- GPU å†…å­˜ 8GB+

### âš™ï¸ å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/qqtang-code/SparseAttn.git
cd SparseAttn

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£… SparseAttn
pip install -e .
```

### ğŸ¬ åŸºç¡€ä½¿ç”¨

#### 1. Xattention ç¨€ç–æ³¨æ„åŠ›

```python
from sparseattn.src.Xattention import Xattention_prefill
import torch

# åˆå§‹åŒ–è¾“å…¥å¼ é‡
batch_size, num_heads, seq_len, head_dim = 1, 32, 4096, 128
query = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
key = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
value = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')

# æ‰§è¡Œç¨€ç–æ³¨æ„åŠ›è®¡ç®—
output = Xattention_prefill(
    query_states=query,
    key_states=key,
    value_states=value,
    threshold=0.95,  # ç¨€ç–åŒ–é˜ˆå€¼
    causal=True      # æ˜¯å¦ä½¿ç”¨å› æœæ©ç 
)
```

#### 2. FlexPrefill å—ç¨€ç–æ³¨æ„åŠ›

```python
from sparseattn.src.Flexprefill import Flexprefill_prefill

# å—ç¨€ç–æ³¨æ„åŠ›è®¡ç®—
output = Flexprefill_prefill(
    query_states=query,
    key_states=key,
    value_states=value,
    block_size=64,      # å—å¤§å°
    sparsity_ratio=0.2  # ç¨€ç–åº¦
)
```

#### 3. Minference è½»é‡çº§æ¨ç†

```python
from sparseattn.src.Minference import Minference_prefill

# è½»é‡çº§æ¨ç†æ¨¡å¼
output = Minference_prefill(
    query_states=query,
    key_states=key,
    value_states=value,
    vertical_size=1000,    # å‚ç›´ç¨€ç–å¤§å°
    slash_size=6096,       # æ–œçº¿ç¨€ç–å¤§å°
    adaptive_budget=0.1    # è‡ªé€‚åº”é¢„ç®—
)
```

### ğŸ”§ é…ç½®æ–‡ä»¶

åˆ›å»ºé…ç½®æ–‡ä»¶ `config/xattn_config.json`ï¼š

```json
{
    "stride": 16,
    "threshold": 0.95,
    "block_size": 64,
    "sparsity_ratio": 0.2,
    "adaptive_budget": 0.1
}
```

## ğŸ“š API æ–‡æ¡£

### æ ¸å¿ƒå‡½æ•°

#### `Xattention_prefill()`
è‡ªé€‚åº”ç¨€ç–æ³¨æ„åŠ›çš„æ ¸å¿ƒå®ç°

**å‚æ•°:**
- `query_states` (torch.Tensor): æŸ¥è¯¢å¼ é‡ [batch, heads, seq_len, head_dim]
- `key_states` (torch.Tensor): é”®å¼ é‡
- `value_states` (torch.Tensor): å€¼å¼ é‡
- `threshold` (float): ç¨€ç–åŒ–é˜ˆå€¼ (0.0-1.0)
- `causal` (bool): æ˜¯å¦ä½¿ç”¨å› æœæ©ç 

**è¿”å›:**
- `torch.Tensor`: æ³¨æ„åŠ›è¾“å‡ºå¼ é‡

### å·¥å…·å‡½æ•°

#### `create_causal_mask()`
åˆ›å»ºå› æœæ³¨æ„åŠ›æ©ç 

#### `find_blocks_chunked()`
åŸºäºé˜ˆå€¼é€‰æ‹©ç›¸å…³æ³¨æ„åŠ›å—

## ğŸ“Š æ€§èƒ½åŸºå‡†

### å†…å­˜ä½¿ç”¨å¯¹æ¯”

| æ¨¡å‹å¤§å° | åºåˆ—é•¿åº¦ | æ ‡å‡†æ³¨æ„åŠ› | SparseAttn | å†…å­˜èŠ‚çœ |
|---------|----------|-----------|-----------|---------|
| 7B      | 4K       | 24GB      | 6GB       | 75%     |
| 13B     | 8K       | 48GB      | 12GB      | 75%     |
| 70B     | 16K      | 192GB     | 38GB      | 80%     |

### é€Ÿåº¦æ€§èƒ½

| æ“ä½œç±»å‹ | æ ‡å‡†å®ç° | SparseAttn | åŠ é€Ÿæ¯” |
|---------|---------|-----------|-------|
| é¢„å¡«å……   | 100ms   | 35ms      | 2.8x  |
| è§£ç      | 50ms    | 18ms      | 2.7x  |

## ğŸ”¬ æŠ€æœ¯åŸç†

### ç¨€ç–åŒ–ç­–ç•¥

1. **é˜ˆå€¼åŸºç¨€ç–åŒ–**: ä¿ç•™æ³¨æ„åŠ›åˆ†æ•°é«˜äºé˜ˆå€¼çš„è¿æ¥
2. **å—çº§ç¨€ç–åŒ–**: ä»¥å—ä¸ºå•ä½è¿›è¡Œç¨€ç–æ“ä½œ
3. **è‡ªé€‚åº”é¢„ç®—**: æ ¹æ®åºåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´ç¨€ç–åº¦

### ä¼˜åŒ–æŠ€æœ¯

- **Triton å†…æ ¸**: å®šåˆ¶åŒ– GPU è®¡ç®—å†…æ ¸
- **å†…å­˜åˆå¹¶**: ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
- **æ•°å€¼ç¨³å®š**: æ”¹è¿›çš„ softmax å’Œå½’ä¸€åŒ–è®¡ç®—

## ğŸ› ï¸ å¼€å‘æŒ‡å—

### é¡¹ç›®ç»“æ„

```
SparseAttn/
â”œâ”€â”€ sparseattn/                 # ä¸»è¦æºç 
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ Xattention.py      # è‡ªé€‚åº”ç¨€ç–æ³¨æ„åŠ›
â”‚   â”‚   â”œâ”€â”€ Flexprefill.py     # çµæ´»é¢„å¡«å……
â”‚   â”‚   â”œâ”€â”€ Minference.py      # è½»é‡çº§æ¨ç†
â”‚   â”‚   â”œâ”€â”€ Fullprefill.py     # å®Œæ•´é¢„å¡«å……
â”‚   â”‚   â”œâ”€â”€ load_llama.py      # LLaMA æ¨¡å‹é›†æˆ
â”‚   â”‚   â””â”€â”€ utils.py           # å·¥å…·å‡½æ•°
â”œâ”€â”€ config/                     # é…ç½®æ–‡ä»¶
â”œâ”€â”€ Block-Sparse-Attention/     # å—ç¨€ç–æ³¨æ„åŠ›
â”œâ”€â”€ requirements.txt           # ä¾èµ–åˆ—è¡¨
â””â”€â”€ pyproject.toml            # é¡¹ç›®é…ç½®
```

### æ·»åŠ æ–°çš„ç¨€ç–åŒ–ç­–ç•¥

1. åœ¨ `sparseattn/src/` ä¸‹åˆ›å»ºæ–°çš„ Python æ–‡ä»¶
2. å®ç°ç¨€ç–åŒ–ç®—æ³•å’Œç›¸åº”çš„ Triton å†…æ ¸
3. åœ¨ `utils.py` ä¸­æ·»åŠ è¾…åŠ©å‡½æ•°
4. ç¼–å†™æµ‹è¯•ç”¨ä¾‹

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºè´¡çŒ®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

### ä»£ç è§„èŒƒ

- éµå¾ª PEP 8 ä»£ç é£æ ¼
- æ·»åŠ é€‚å½“çš„æ–‡æ¡£å­—ç¬¦ä¸²
- ç¼–å†™å•å…ƒæµ‹è¯•
- ç¡®ä¿å‘åå…¼å®¹æ€§

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- [FlashAttention](https://github.com/Dao-AILab/flash-attention) - é«˜æ•ˆæ³¨æ„åŠ›å®ç°çš„å¯å‘
- [Triton](https://github.com/openai/triton) - GPU å†…æ ¸å¼€å‘æ¡†æ¶
- [Transformers](https://github.com/huggingface/transformers) - æ¨¡å‹å®ç°åŸºç¡€

## ğŸ“ è”ç³»æˆ‘ä»¬

- ğŸ› é—®é¢˜åé¦ˆ: [GitHub Issues](https://github.com/qqtang-code/SparseAttn/issues)
- ğŸ’¬ è®¨è®ºäº¤æµ: [GitHub Discussions](https://github.com/qqtang-code/SparseAttn/discussions)
- ğŸ“§ é‚®ç®±è”ç³»: your-email@example.com

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼**

Made with â¤ï¸ by the SparseAttn Team

</div>