# ğŸš€ SparseAttn

<div align="center">

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)
![PyTorch](https://img.shields.io/badge/pytorch-2.0%2B-red.svg)
![CUDA](https://img.shields.io/badge/CUDA-11.8%2B-green.svg)

**é«˜æ€§èƒ½ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åº“ - ä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æä¾›é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—**

[åŠŸèƒ½ç‰¹æ€§](#-åŠŸèƒ½ç‰¹æ€§) â€¢ [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) â€¢ [API æ–‡æ¡£](#-api-æ–‡æ¡£) â€¢ [æ€§èƒ½åŸºå‡†](#-æ€§èƒ½åŸºå‡†) â€¢ [è´¡çŒ®æŒ‡å—](#-è´¡çŒ®æŒ‡å—)

</div>

## ğŸ“– ç®€ä»‹

SparseAttn æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®¾è®¡çš„é«˜æ€§èƒ½ç¨€ç–æ³¨æ„åŠ›åº“ã€‚é€šè¿‡å…ˆè¿›çš„ç¨€ç–åŒ–æŠ€æœ¯å’Œ GPU ä¼˜åŒ–ï¼Œæ˜¾è‘—é™ä½æ³¨æ„åŠ›è®¡ç®—çš„å†…å­˜æ¶ˆè€—å’Œè®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

### ğŸ¯ ä¸»è¦ä¼˜åŠ¿

- **ğŸ”¥ é«˜æ€§èƒ½**: åŸºäº Triton çš„è‡ªå®šä¹‰ CUDA å†…æ ¸ï¼Œæä¾›æè‡´æ€§èƒ½
- **ğŸ’¾ å†…å­˜é«˜æ•ˆ**: ç›¸æ¯”ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼Œå†…å­˜ä½¿ç”¨é‡å‡å°‘ 80%+
- **ğŸ›ï¸ çµæ´»é…ç½®**: æ”¯æŒå¤šç§ç¨€ç–åŒ–ç­–ç•¥å’Œå‚æ•°è°ƒä¼˜
- **ğŸ”§ æ˜“äºé›†æˆ**: ä¸ Transformers ç”Ÿæ€ç³»ç»Ÿæ— ç¼é›†æˆ
- **ğŸ“Š å¤šç§æ¨¡å¼**: æ”¯æŒé¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„ä¸åŒä¼˜åŒ–ç­–ç•¥

## âœ¨ åŠŸèƒ½ç‰¹æ€§

### ğŸ—ï¸ æ ¸å¿ƒç»„ä»¶

#### 1. **Xattention** - è‡ªé€‚åº”ç¨€ç–æ³¨æ„åŠ›
- åŸºäºé˜ˆå€¼çš„åŠ¨æ€ç¨€ç–åŒ–
- æ”¯æŒå› æœå’Œéå› æœæ³¨æ„åŠ›
- é«˜åº¦ä¼˜åŒ–çš„ Triton å†…æ ¸å®ç°

#### 2. **FlexPrefill** - çµæ´»é¢„å¡«å……ç­–ç•¥
- å—çº§ç¨€ç–æ³¨æ„åŠ›
- è‡ªé€‚åº”å—é€‰æ‹©ç®—æ³•
- é•¿åºåˆ—çš„é«˜æ•ˆå¤„ç†

#### 3. **Minference** - è½»é‡çº§æ¨ç†
- å‚ç›´å’Œå¯¹è§’ç¨€ç–æ¨¡å¼
- è‡ªé€‚åº”é¢„ç®—åˆ†é…
- ä¸“ä¸ºæ¨ç†é˜¶æ®µä¼˜åŒ–

#### 4. **FullPrefill** - å®Œæ•´é¢„å¡«å……
- åŸºäº FlashInfer çš„é«˜æ•ˆå®ç°
- æ”¯æŒè‡ªå®šä¹‰æ©ç 
- å†…å­˜å’Œè®¡ç®—çš„åŒé‡ä¼˜åŒ–

### ğŸ‹ï¸ è®­ç»ƒæ”¯æŒ
- **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒå¤šGPUå’Œå¤šèŠ‚ç‚¹è®­ç»ƒï¼Œå…·å¤‡åºåˆ—å¹¶è¡Œèƒ½åŠ›
- **ç¨€ç–å¾®è°ƒ**: æ”¯æŒåœ¨è¯­è¨€æ¨¡å‹ä¸Šè®­ç»ƒç¨€ç–æ³¨æ„åŠ›æ¨¡å¼çš„æ–¹æ³•
- **çµæ´»ç¨€ç–æ§åˆ¶**: å¯é…ç½®çš„ç¨€ç–æ¯”ä¾‹å’Œæ¨¡å¼
- **æ©ç å­¦ä¹ **: ä¸“é—¨è®­ç»ƒæ³¨æ„åŠ›æ©ç ï¼Œæ”¯æŒç‹¬ç«‹å­¦ä¹ ç‡
- **æ­£åˆ™åŒ–æŠ€æœ¯**: æ”¯æŒå¤šç§ç¨€ç–æ€§æ§åˆ¶çš„æ­£åˆ™åŒ–æ–¹æ³•

### ğŸ”§ æŠ€æœ¯ç‰¹æ€§

- **ğŸ¯ æ™ºèƒ½ç¨€ç–åŒ–**: åŸºäºæ³¨æ„åŠ›åˆ†æ•°çš„è‡ªé€‚åº”ç¨€ç–æ¨¡å¼
- **âš¡ GPU åŠ é€Ÿ**: å®Œå…¨åŸºäº CUDA çš„é«˜æ€§èƒ½å®ç°
- **ğŸ§© æ¨¡å—åŒ–è®¾è®¡**: å¯æ’æ‹”çš„æ³¨æ„åŠ›ç»„ä»¶
- **ğŸ“ˆ å¯æ‰©å±•æ€§**: æ”¯æŒä»å°å‹åˆ°è¶…å¤§å‹æ¨¡å‹çš„å„ç§è§„æ¨¡
- **ğŸ”’ æ•°å€¼ç¨³å®šæ€§**: ç²¾å¿ƒè®¾è®¡çš„æ•°å€¼è®¡ç®—ï¼Œç¡®ä¿è®­ç»ƒç¨³å®šæ€§

## ğŸ“ é¡¹ç›®ç»“æ„

```
SparseAttn/
â”œâ”€â”€ sparseattn/              # ä¸»åŒ…
â”‚   â”œâ”€â”€ __init__.py          # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ training/            # ç¨€ç–æ³¨æ„åŠ›è®­ç»ƒæ¨¡å—
â”‚   â”œâ”€â”€ threshold/           # åŸºäºé˜ˆå€¼çš„ç¨€ç–æ³¨æ„åŠ›æ¨¡å—
â”‚   â”œâ”€â”€ run_scripts/         # è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ src/                 # æ ¸å¿ƒæºä»£ç 
â”‚       â”œâ”€â”€ __init__.py      # æºåŒ…åˆå§‹åŒ–
â”‚       â”œâ”€â”€ Xattention.py    # Xattention å®ç°
â”‚       â”œâ”€â”€ Flexprefill.py   # FlexPrefill å®ç°
â”‚       â”œâ”€â”€ Minference.py    # Minference å®ç°
â”‚       â”œâ”€â”€ Fullprefill.py   # FullPrefill å®ç°
â”‚       â”œâ”€â”€ load_llama.py    # LLaMA æ¨¡å‹åŠ è½½å·¥å…·
â”‚       â””â”€â”€ utils.py         # å·¥å…·å‡½æ•°
â”œâ”€â”€ config/                  # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ xattn_config.json    # é»˜è®¤é…ç½®
â”œâ”€â”€ examples/                # ç¤ºä¾‹ä½¿ç”¨è„šæœ¬
â”œâ”€â”€ tests/                   # å•å…ƒæµ‹è¯•
â”œâ”€â”€ docs/                    # æ–‡æ¡£
â”œâ”€â”€ third_party/             # ç¬¬ä¸‰æ–¹ä¾èµ–
â”œâ”€â”€ requirements.txt         # Python ä¾èµ–
â”œâ”€â”€ pyproject.toml           # åŒ…é…ç½®
â””â”€â”€ README.md                # è¯´æ˜æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ğŸ“‹ ç¯å¢ƒè¦æ±‚

- Python 3.10+
- PyTorch 2.4+
- CUDA 12.4+
- GPU å†…å­˜ 24GB+

### âš™ï¸ å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/qqtang-code/SparseAttn.git
cd SparseAttn

# å®‰è£…ä¾èµ–
pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 -f https://mirrors.aliyun.com/pytorch-wheels/cu124
pip install flashinfer-python -i https://flashinfer.ai/whl/cu124/torch2.5/
git clone https://gitee.com/codingQQT/Block-Sparse-Attention.git
cd Block-Sparse-Attention && CUDA_HOME=/usr/local/cuda-12.4/ python setup.py install
pip install -r requirements.txt

# å®‰è£… SparseAttn
pip install -e .
```

### ğŸ¬ åŸºæœ¬ä½¿ç”¨

#### 1. Xattention ç¨€ç–æ³¨æ„åŠ›

```python
from sparseattn.src.Xattention import Xattention_prefill
import torch

# åˆå§‹åŒ–è¾“å…¥å¼ é‡
batch_size, num_heads, seq_len, head_dim = 1, 32, 4096, 128
query = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
key = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
value = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')

# æ‰§è¡Œç¨€ç–æ³¨æ„åŠ›è®¡ç®—
output = Xattention_prefill(
    query_states=query,
    key_states=key,
    value_states=value,
    threshold=0.95,  # ç¨€ç–åŒ–é˜ˆå€¼
    causal=True      # æ˜¯å¦ä½¿ç”¨å› æœæ©ç 
)
```

#### 2. FlexPrefill å—ç¨€ç–æ³¨æ„åŠ›

```python
from sparseattn.src.Flexprefill import Flexprefill_prefill

# å—ç¨€ç–æ³¨æ„åŠ›è®¡ç®—
output = Flexprefill_prefill(
    query_states=query,
    key_states=key,
    value_states=value,
    block_size=64,      # å—å¤§å°
    sparsity_ratio=0.2  # ç¨€ç–æ¯”ä¾‹
)
```

#### 3. Minference è½»é‡çº§æ¨ç†

```python
from sparseattn.src.Minference import Minference_prefill

# è½»é‡çº§æ¨ç†æ¨¡å¼
output = Minference_prefill(
    query_states=query,
    key_states=key,
    value_states=value,
    vertical_size=1000,    # å‚ç›´ç¨€ç–å¤§å°
    slash_size=6096,       # å¯¹è§’ç¨€ç–å¤§å°
    adaptive_budget=0.1    # è‡ªé€‚åº”é¢„ç®—
)
```

### ğŸ‹ï¸ è®­ç»ƒä½¿ç”¨

#### 1. ç¨€ç–å¾®è°ƒ

```bash
# ä½¿ç”¨å­¦ä¹ çš„æ©ç å’Œæƒé‡è¿›è¡Œå¾®è°ƒ
cd sparseattn/run_scripts
bash prulong_masksandweights.sh

# ä½¿ç”¨å›ºå®šæ©ç è¿›è¡Œå¾®è°ƒï¼ˆä»…è®­ç»ƒæƒé‡ï¼‰
bash prulong_masksonly.sh

# æ ‡å‡†SFTåŸºçº¿
bash sft.sh
```

#### 2. è®­ç»ƒé…ç½®

å…³é”®è®­ç»ƒå‚æ•°ï¼š
- `start_head_sparsity`: æ³¨æ„åŠ›å¤´çš„åˆå§‹ç¨€ç–æ¯”ä¾‹
- `end_head_sparsity`: æ³¨æ„åŠ›å¤´çš„æœ€ç»ˆç¨€ç–æ¯”ä¾‹
- `mask_learning_rate`: æ©ç å‚æ•°çš„å­¦ä¹ ç‡
- `reg_learning_rate`: æ­£åˆ™åŒ–å‚æ•°çš„å­¦ä¹ ç‡
- `sparsity_warmup_ratio`: ç¨€ç–é¢„çƒ­çš„è®­ç»ƒæ­¥æ•°æ¯”ä¾‹
- `seq_parallel_size`: åˆ†å¸ƒå¼è®­ç»ƒçš„åºåˆ—å¹¶è¡Œåº¦

### ğŸ”§ é…ç½®æ–‡ä»¶

åˆ›å»ºé…ç½®æ–‡ä»¶ `config/xattn_config.json`:

```json
{
    "stride": 16,
    "threshold": 0.95,
    "block_size": 64,
    "sparsity_ratio": 0.2,
    "adaptive_budget": 0.1
}
```

## ğŸ“š API æ–‡æ¡£

### Xattention

Xattention æä¾›åŸºäºé˜ˆå€¼çš„è‡ªé€‚åº”ç¨€ç–æ³¨æ„åŠ›è®¡ç®—ã€‚

```python
def Xattention_prefill(
    query_states: torch.Tensor,
    key_states: torch.Tensor,
    value_states: torch.Tensor,
    threshold: float = 0.95,
    causal: bool = True
) -> torch.Tensor
```

å‚æ•°:
- `query_states`: æŸ¥è¯¢å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_heads, seq_len, head_dim]
- `key_states`: é”®å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_heads, seq_len, head_dim]
- `value_states`: å€¼å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_heads, seq_len, head_dim]
- `threshold`: ç¨€ç–åŒ–é˜ˆå€¼ï¼ˆé»˜è®¤: 0.95ï¼‰
- `causal`: æ˜¯å¦åº”ç”¨å› æœæ©ç ï¼ˆé»˜è®¤: Trueï¼‰

è¿”å›:
- ä¸è¾“å…¥å¼ é‡å½¢çŠ¶ç›¸åŒçš„è¾“å‡ºå¼ é‡

### FlexPrefill

FlexPrefill å®ç°å…·æœ‰è‡ªé€‚åº”å—é€‰æ‹©çš„å—çº§ç¨€ç–æ³¨æ„åŠ›ã€‚

```python
def Flexprefill_prefill(
    query_states: torch.Tensor,
    key_states: torch.Tensor,
    value_states: torch.Tensor,
    block_size: int = 64,
    sparsity_ratio: float = 0.2
) -> torch.Tensor
```

å‚æ•°:
- `query_states`: æŸ¥è¯¢å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_heads, seq_len, head_dim]
- `key_states`: é”®å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_heads, seq_len, head_dim]
- `value_states`: å€¼å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_heads, seq_len, head_dim]
- `block_size`: æ¯ä¸ªå—çš„å¤§å°ï¼ˆé»˜è®¤: 64ï¼‰
- `sparsity_ratio`: é€‰æ‹©çš„å—æ¯”ä¾‹ï¼ˆé»˜è®¤: 0.2ï¼‰

è¿”å›:
- ä¸è¾“å…¥å¼ é‡å½¢çŠ¶ç›¸åŒçš„è¾“å‡ºå¼ é‡

### Minference

Minference æä¾›å…·æœ‰å‚ç›´å’Œå¯¹è§’ç¨€ç–æ¨¡å¼çš„è½»é‡çº§æ¨ç†ã€‚

```python
def Minference_prefill(
    query_states: torch.Tensor,
    key_states: torch.Tensor,
    value_states: torch.Tensor,
    vertical_size: int = 1000,
    slash_size: int = 6096,
    adaptive_budget: float = None
) -> torch.Tensor
```

å‚æ•°:
- `query_states`: æŸ¥è¯¢å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_heads, seq_len, head_dim]
- `key_states`: é”®å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_heads, seq_len, head_dim]
- `value_states`: å€¼å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_heads, seq_len, head_dim]
- `vertical_size`: å‚ç›´ç¨€ç–æ¨¡å¼çš„å¤§å°ï¼ˆé»˜è®¤: 1000ï¼‰
- `slash_size`: å¯¹è§’ç¨€ç–æ¨¡å¼çš„å¤§å°ï¼ˆé»˜è®¤: 6096ï¼‰
- `adaptive_budget`: è‡ªé€‚åº”é¢„ç®—æ¯”ä¾‹ï¼ˆé»˜è®¤: Noneï¼‰

è¿”å›:
- ä¸è¾“å…¥å¼ é‡å½¢çŠ¶ç›¸åŒçš„è¾“å‡ºå¼ é‡

### FullPrefill

FullPrefill æä¾›åŸºäº FlashInfer çš„å®Œæ•´é¢„å¡«å……å®ç°ã€‚

```python
def Full_prefill(
    query_states: torch.Tensor,
    key_states: torch.Tensor,
    value_states: torch.Tensor,
    causal: bool = True,
    attention_mask = None
) -> torch.Tensor
```

å‚æ•°:
- `query_states`: æŸ¥è¯¢å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_heads, seq_len, head_dim]
- `key_states`: é”®å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_heads, seq_len, head_dim]
- `value_states`: å€¼å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_heads, seq_len, head_dim]
- `causal`: æ˜¯å¦åº”ç”¨å› æœæ©ç ï¼ˆé»˜è®¤: Trueï¼‰
- `attention_mask`: è‡ªå®šä¹‰æ³¨æ„åŠ›æ©ç ï¼ˆé»˜è®¤: Noneï¼‰

è¿”å›:
- ä¸è¾“å…¥å¼ é‡å½¢çŠ¶ç›¸åŒçš„è¾“å‡ºå¼ é‡